{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562b56de-d1f0-46cd-a5d6-9decf518eb51",
   "metadata": {},
   "source": [
    "# PyTorch Lightning\n",
    "\n",
    "We want to use PyTorch Lightning  now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1edf75-99e3-40ed-984b-2d182849d6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "\n",
    "# check if cuda is usable\n",
    "if torch.cuda.is_available():\n",
    "  device = 'cuda' \n",
    "else:\n",
    "  device = 'cpu' \n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c597b25-679c-44cc-8cc6-6de99ecfb0b9",
   "metadata": {},
   "source": [
    "## Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c05b983-5595-41d8-9562-e206bbea28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDogImageDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dl_path='./tmp', batch_size = 32, num_workers=0, cache_dataset=True):\n",
    "        super().__init__()\n",
    "        self._dl_path = dl_path\n",
    "        self.batch_size = batch_size\n",
    "        self._num_workers = num_workers\n",
    "        self._cache_dataset = cache_dataset\n",
    "\n",
    "    @property\n",
    "    def data_path(self):\n",
    "        return Path(self._dl_path).joinpath(\"PetImages\")\n",
    "\n",
    "    @property\n",
    "    def normalize_transform(self):\n",
    "        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    @property\n",
    "    def train_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            self.normalize_transform,\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def val_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)), \n",
    "            transforms.ToTensor(), \n",
    "            self.normalize_transform\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Download images and prepare images datasets.\"\"\"\n",
    "        url = 'https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip'  \n",
    "        if not os.path.exists(self.data_path):\n",
    "            os.makedirs(self.data_path)\n",
    "        if len(os.listdir(self.data_path)) == 0:\n",
    "            download_and_extract_archive(url=url, download_root=self._dl_path, remove_finished=not self._cache_dataset)\n",
    "        else:\n",
    "            print(\"Dataset already exists, skipping download and extraction...\")\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        # make assignments here (val/train/test split)\n",
    "        dataset = self.create_dataset(self.data_path, self.train_transform)\n",
    "        self.train_data, self.val_data = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "        print(\"Dataset created, split:\")\n",
    "        print(f'training images: {len(self.train_data)}')\n",
    "        print(f'validation images: {len(self.val_data)}')\n",
    "\n",
    "    def create_dataset(self, root, transform):\n",
    "        return ImageFolder(root=root, transform=transform, is_valid_file=self._is_image_valid)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset=self.train_data, batch_size=self.batch_size, num_workers=self._num_workers, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset=self.val_data, batch_size=self.batch_size, num_workers=self._num_workers, shuffle=False)\n",
    "\n",
    "    def _is_image_valid(self, image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caa1a91-9047-468f-8a36-4a5fc242290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = CatDogImageDataModule(num_workers=16, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e61e4a8-b2b3-48c6-96bb-d9176d6b90f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download and extraction...\n"
     ]
    }
   ],
   "source": [
    "# The following methods will be called by the trainer automatically before training:\n",
    "dm.prepare_data()\n",
    "#dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8e92f-c7cb-4821-bd2d-6da2b2e23da8",
   "metadata": {},
   "source": [
    "## Build Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8eda0e1-a4fc-4b4c-ba8e-14b735f6bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(pl.LightningModule):\n",
    "    def __init__(self, lr=0.02):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.conv_1 = nn.Sequential(nn.Conv2d(3, 16, 3), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv_2 = nn.Sequential(nn.Conv2d(16, 32, 3), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv_3 = nn.Sequential(nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.fc_1 = nn.Sequential(nn.Flatten(), nn.Linear(43264,256), nn.ReLU(), nn.Linear(256,128), nn.ReLU())\n",
    "        self.fc_2 = nn.Sequential(nn.Linear(128,2),)\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "      return F.nll_loss(logits, labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, label = batch\n",
    "        output = self.forward(data)\n",
    "        loss = nn.CrossEntropyLoss()(output,label)\n",
    "        self.log('train_loss', loss)\n",
    "        return {'loss': loss, 'log': self.log}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_data, val_label = batch\n",
    "        val_output = self.forward(val_data)\n",
    "        val_loss = nn.CrossEntropyLoss()(val_output, val_label)\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        return F.softmax(x,dim = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0410169-183d-4e71-b23f-18c067154005",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d22a770-d8b7-46de-93fa-38a8c3926a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69337d-dadb-4bd0-8528-da158ede7129",
   "metadata": {},
   "source": [
    "## Setup trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "103dde40-910a-4669-95a9-95a09c58b0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f2068-a556-4226-804c-d7e879c269c1",
   "metadata": {},
   "source": [
    "Finding Batch size automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddd831c7-9dbb-40c7-8eed-89afa48addc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner = Tuner(trainer)\n",
    "\n",
    "# Auto-scale batch size by growing it exponentially (default)\n",
    "#tuner.scale_batch_size(model, mode=\"power\", datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8d8b4-a6ad-45e3-9652-a1885d683c72",
   "metadata": {},
   "source": [
    "Learning rate tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f0bf37c-453b-4c29-9ff5-1be1c4b8c602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download and extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created, split:\n",
      "training images: 19999\n",
      "validation images: 4999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3771bffdfdca43569987d25d8b1f3768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=500` reached.\n",
      "Learning rate set to 0.00039084089579240173\n",
      "Restoring states from the checkpoint path at /app/notebooks/.lr_find_bed6ed20-cc70-4b95-a407-6256ba7dd136.ckpt\n",
      "Restored all states from the checkpoint at /app/notebooks/.lr_find_bed6ed20-cc70-4b95-a407-6256ba7dd136.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner = Tuner(trainer)\n",
    "\n",
    "# Auto-scale batch size by growing it exponentially (default)\n",
    "lr_finder = tuner.lr_find(model, mode=\"exponential\", datamodule=dm, num_training=500)\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e490c65-3bb0-4e07-82e9-c07558a8c818",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d85a16b0-3ef7-4275-b171-1bb9335e9e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download and extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | conv_1 | Sequential | 448   \n",
      "1 | conv_2 | Sequential | 4.6 K \n",
      "2 | conv_3 | Sequential | 18.5 K\n",
      "3 | fc_1   | Sequential | 11.1 M\n",
      "4 | fc_2   | Sequential | 258   \n",
      "--------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.530    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created, split:\n",
      "training images: 19999\n",
      "validation images: 4999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9184a4c0bb4db1a7300b883e658ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7cb24-b98e-4d99-8a00-e4535afb447d",
   "metadata": {},
   "source": [
    "## Tensorboard logs\n",
    "We can use tensorboard to visualize our training metrics.\n",
    "\n",
    "We can either manually start the tensorboard server and go to [localhost:6006](http://localhost:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0e871b-f83e-4629-b851-22329ed6d26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.13.0 at http://27b367440664:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir='./lightning_logs' --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1203a-861a-4dbc-b222-8df1de30ec5a",
   "metadata": {},
   "source": [
    "The other possibility is to use the tensorboard extension directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5f8c0d-f55e-4fd4-b05e-fb4358bbeacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bacf954802c3e7c9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bacf954802c3e7c9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53ff99-5456-4097-af55-a1f39344295d",
   "metadata": {},
   "source": [
    "## Use profiler to find bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "858ac0f4-9352-4862-94d0-b66bdab29a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download and extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | conv_1 | Sequential | 448   \n",
      "1 | conv_2 | Sequential | 4.6 K \n",
      "2 | conv_3 | Sequential | 18.5 K\n",
      "3 | fc_1   | Sequential | 11.1 M\n",
      "4 | fc_2   | Sequential | 258   \n",
      "--------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.530    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created, split:\n",
      "training images: 19999\n",
      "validation images: 4999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bc0a3ba9154a289937b3d1191526a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "FIT Profiler Report\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Action                                                                                                                                                         \t|  Mean duration (s)\t|  Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Total                                                                                                                                                          \t|  -              \t|  6336           \t|  382.34         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  run_training_epoch                                                                                                                                             \t|  199.86         \t|  1              \t|  199.86         \t|  52.273         \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.setup                                                                                                               \t|  165.54         \t|  1              \t|  165.54         \t|  43.296         \t|\n",
      "|  run_training_batch                                                                                                                                             \t|  1.0434         \t|  157            \t|  163.81         \t|  42.844         \t|\n",
      "|  [LightningModule]SimpleCNN.optimizer_step                                                                                                                      \t|  1.0432         \t|  157            \t|  163.78         \t|  42.837         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.backward                                                                                                                        \t|  0.56283        \t|  157            \t|  88.365         \t|  23.112         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.training_step                                                                                                                   \t|  0.45052        \t|  157            \t|  70.731         \t|  18.499         \t|\n",
      "|  [_EvaluationLoop].val_next                                                                                                                                     \t|  0.44695        \t|  43             \t|  19.219         \t|  5.0266         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.validation_step                                                                                                                 \t|  0.4565         \t|  42             \t|  19.173         \t|  5.0146         \t|\n",
      "|  [_TrainingEpochLoop].train_dataloader_next                                                                                                                     \t|  0.045153       \t|  157            \t|  7.089          \t|  1.8541         \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_end       \t|  0.29569        \t|  1              \t|  0.29569        \t|  0.077335       \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_end                                                                                                                   \t|  0.0015746      \t|  157            \t|  0.24722        \t|  0.064659       \t|\n",
      "|  [LightningModule]SimpleCNN.optimizer_zero_grad                                                                                                                 \t|  0.00079984     \t|  157            \t|  0.12557        \t|  0.032843       \t|\n",
      "|  [Strategy]SingleDeviceStrategy.batch_to_device                                                                                                                 \t|  0.00023259     \t|  199            \t|  0.046285       \t|  0.012106       \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_batch_end                                                                                                              \t|  0.0010914      \t|  42             \t|  0.04584        \t|  0.011989       \t|\n",
      "|  [LightningModule]SimpleCNN.transfer_batch_to_device                                                                                                            \t|  0.00012917     \t|  199            \t|  0.025704       \t|  0.0067228      \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_start                                                                                                                  \t|  0.0066992      \t|  2              \t|  0.013398       \t|  0.0035043      \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_batch_start                                                                                                            \t|  0.00026789     \t|  42             \t|  0.011251       \t|  0.0029428      \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.prepare_data                                                                                                        \t|  0.010801       \t|  1              \t|  0.010801       \t|  0.0028249      \t|\n",
      "|  [Callback]TQDMProgressBar.on_sanity_check_start                                                                                                                \t|  0.0090482      \t|  1              \t|  0.0090482      \t|  0.0023665      \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_start                                                                                                                       \t|  0.0077118      \t|  1              \t|  0.0077118      \t|  0.002017       \t|\n",
      "|  [LightningModule]SimpleCNN.configure_gradient_clipping                                                                                                         \t|  4.4046e-05     \t|  157            \t|  0.0069153      \t|  0.0018087      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_end       \t|  3.7259e-05     \t|  157            \t|  0.0058497      \t|  0.00153        \t|\n",
      "|  [Callback]ModelSummary.on_fit_start                                                                                                                            \t|  0.00262        \t|  1              \t|  0.00262        \t|  0.00068524     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_end                                                                                                                    \t|  0.0011806      \t|  2              \t|  0.0023612      \t|  0.00061757     \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_end                                                                                                                      \t|  1.0223e-05     \t|  157            \t|  0.001605       \t|  0.00041979     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.setup                    \t|  0.0015413      \t|  1              \t|  0.0015413      \t|  0.00040313     \t|\n",
      "|  [Callback]TQDMProgressBar.on_after_backward                                                                                                                    \t|  9.0288e-06     \t|  157            \t|  0.0014175      \t|  0.00037075     \t|\n",
      "|  [LightningModule]SimpleCNN.on_before_batch_transfer                                                                                                            \t|  4.9639e-06     \t|  199            \t|  0.00098782     \t|  0.00025836     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_model_eval                                                                                                            \t|  0.0004933      \t|  2              \t|  0.0009866      \t|  0.00025804     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_start                                                                                                                 \t|  5.8494e-06     \t|  157            \t|  0.00091836     \t|  0.00024019     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_zero_grad                                                                                                                  \t|  4.8144e-06     \t|  157            \t|  0.00075585     \t|  0.00019769     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_backward                                                                                                                   \t|  4.3077e-06     \t|  157            \t|  0.0006763      \t|  0.00017688     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_optimizer_step                                                                                                             \t|  3.9552e-06     \t|  157            \t|  0.00062096     \t|  0.00016241     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_backward       \t|  3.9271e-06     \t|  157            \t|  0.00061655     \t|  0.00016126     \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.val_dataloader                                                                                                      \t|  0.00060321     \t|  1              \t|  0.00060321     \t|  0.00015777     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_start                                                                                                                 \t|  0.00059927     \t|  1              \t|  0.00059927     \t|  0.00015674     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_start     \t|  3.2965e-06     \t|  157            \t|  0.00051755     \t|  0.00013536     \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_start                                                                                                                    \t|  3.2854e-06     \t|  157            \t|  0.00051581     \t|  0.00013491     \t|\n",
      "|  [LightningModule]SimpleCNN.on_after_batch_transfer                                                                                                             \t|  2.5381e-06     \t|  199            \t|  0.00050509     \t|  0.0001321      \t|\n",
      "|  [Callback]ModelSummary.on_before_zero_grad                                                                                                                     \t|  3.0525e-06     \t|  157            \t|  0.00047925     \t|  0.00012535     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_end                                                                                                                         \t|  0.00046157     \t|  1              \t|  0.00046157     \t|  0.00012072     \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.train_dataloader                                                                                                    \t|  0.00040584     \t|  1              \t|  0.00040584     \t|  0.00010614     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_end                                                                                                                   \t|  0.00039762     \t|  1              \t|  0.00039762     \t|  0.000104       \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_after_backward        \t|  2.4104e-06     \t|  157            \t|  0.00037843     \t|  9.8978e-05     \t|\n",
      "|  [Callback]ModelSummary.on_after_backward                                                                                                                       \t|  2.3845e-06     \t|  157            \t|  0.00037436     \t|  9.7914e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_batch_start                                                                                                                \t|  2.3571e-06     \t|  157            \t|  0.00037006     \t|  9.6789e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_batch_end                                                                                                                  \t|  2.3416e-06     \t|  157            \t|  0.00036764     \t|  9.6154e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_before_zero_grad                                                                                                                 \t|  2.3214e-06     \t|  157            \t|  0.00036446     \t|  9.5322e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_zero_grad      \t|  2.2854e-06     \t|  157            \t|  0.00035881     \t|  9.3845e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_after_backward                                                                                                                   \t|  2.2716e-06     \t|  157            \t|  0.00035665     \t|  9.328e-05      \t|\n",
      "|  [Callback]ModelSummary.on_before_backward                                                                                                                      \t|  2.1938e-06     \t|  157            \t|  0.00034442     \t|  9.0082e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_model_train                                                                                                           \t|  0.00016758     \t|  2              \t|  0.00033517     \t|  8.7662e-05     \t|\n",
      "|  [Callback]ModelSummary.on_validation_batch_end                                                                                                                 \t|  7.7705e-06     \t|  42             \t|  0.00032636     \t|  8.5359e-05     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_batch_start                                                                                                            \t|  2.0699e-06     \t|  157            \t|  0.00032498     \t|  8.4996e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_optimizer_step \t|  2.0514e-06     \t|  157            \t|  0.00032206     \t|  8.4234e-05     \t|\n",
      "|  [Callback]ModelSummary.on_before_optimizer_step                                                                                                                \t|  2.0042e-06     \t|  157            \t|  0.00031466     \t|  8.2298e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_before_backward                                                                                                                  \t|  1.7974e-06     \t|  157            \t|  0.00028219     \t|  7.3807e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.configure_optimizers                                                                                                                \t|  0.00022603     \t|  1              \t|  0.00022603     \t|  5.9118e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_before_optimizer_step                                                                                                            \t|  1.2659e-06     \t|  157            \t|  0.00019875     \t|  5.1982e-05     \t|\n",
      "|  [Callback]ModelSummary.on_validation_batch_start                                                                                                               \t|  4.1054e-06     \t|  42             \t|  0.00017243     \t|  4.5098e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_end        \t|  8.4052e-05     \t|  2              \t|  0.0001681      \t|  4.3967e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_end  \t|  2.4945e-06     \t|  42             \t|  0.00010477     \t|  2.7402e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_start\t|  2.4063e-06     \t|  42             \t|  0.00010107     \t|  2.6433e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_batch_end                                                                                                             \t|  2.3547e-06     \t|  42             \t|  9.8897e-05     \t|  2.5866e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_batch_start                                                                                                           \t|  2.0254e-06     \t|  42             \t|  8.5067e-05     \t|  2.2249e-05     \t|\n",
      "|  [Callback]ModelSummary.on_save_checkpoint                                                                                                                      \t|  3.6856e-05     \t|  1              \t|  3.6856e-05     \t|  9.6396e-06     \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_end                                                                                                                      \t|  2.6706e-05     \t|  1              \t|  2.6706e-05     \t|  6.9849e-06     \t|\n",
      "|  [Callback]ModelSummary.on_validation_start                                                                                                                     \t|  8.923e-06      \t|  2              \t|  1.7846e-05     \t|  4.6676e-06     \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.state_dict                                                                                                          \t|  1.6474e-05     \t|  1              \t|  1.6474e-05     \t|  4.3087e-06     \t|\n",
      "|  [Callback]ModelSummary.on_validation_end                                                                                                                       \t|  5.533e-06      \t|  2              \t|  1.1066e-05     \t|  2.8943e-06     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_start                                                                                                             \t|  5.406e-06      \t|  2              \t|  1.0812e-05     \t|  2.8278e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_sanity_check_end                                                                                                                  \t|  9.564e-06      \t|  1              \t|  9.564e-06      \t|  2.5014e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_epoch_end                                                                                                              \t|  4.285e-06      \t|  2              \t|  8.57e-06       \t|  2.2415e-06     \t|\n",
      "|  [Callback]ModelSummary.on_sanity_check_start                                                                                                                   \t|  7.48e-06       \t|  1              \t|  7.48e-06       \t|  1.9564e-06     \t|\n",
      "|  [Callback]ModelSummary.on_train_start                                                                                                                          \t|  7.47e-06       \t|  1              \t|  7.47e-06       \t|  1.9538e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_save_checkpoint                                                                                                                   \t|  7.412e-06      \t|  1              \t|  7.412e-06      \t|  1.9386e-06     \t|\n",
      "|  [LightningModule]SimpleCNN.configure_callbacks                                                                                                                 \t|  6.752e-06      \t|  1              \t|  6.752e-06      \t|  1.766e-06      \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_start                                                                                                                      \t|  6.024e-06      \t|  1              \t|  6.024e-06      \t|  1.5756e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.setup                                                                                                                                \t|  5.458e-06      \t|  1              \t|  5.458e-06      \t|  1.4275e-06     \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_start                                                                                                                    \t|  4.828e-06      \t|  1              \t|  4.828e-06      \t|  1.2627e-06     \t|\n",
      "|  [LightningModule]SimpleCNN.prepare_data                                                                                                                        \t|  4.716e-06      \t|  1              \t|  4.716e-06      \t|  1.2335e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_start      \t|  2.2855e-06     \t|  2              \t|  4.571e-06      \t|  1.1955e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_end  \t|  2.2675e-06     \t|  2              \t|  4.535e-06      \t|  1.1861e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_start           \t|  4.469e-06      \t|  1              \t|  4.469e-06      \t|  1.1689e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_end                                                                                                                           \t|  4.356e-06      \t|  1              \t|  4.356e-06      \t|  1.1393e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_epoch_start                                                                                                            \t|  1.989e-06      \t|  2              \t|  3.978e-06      \t|  1.0404e-06     \t|\n",
      "|  [Callback]ModelSummary.on_train_end                                                                                                                            \t|  3.877e-06      \t|  1              \t|  3.877e-06      \t|  1.014e-06      \t|\n",
      "|  [Callback]ModelSummary.on_validation_epoch_end                                                                                                                 \t|  1.93e-06       \t|  2              \t|  3.86e-06       \t|  1.0096e-06     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_end                                                                                                                   \t|  1.878e-06      \t|  2              \t|  3.756e-06      \t|  9.8237e-07     \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_start                                                                                                                         \t|  3.336e-06      \t|  1              \t|  3.336e-06      \t|  8.7252e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_save_checkpoint       \t|  3.26e-06       \t|  1              \t|  3.26e-06       \t|  8.5264e-07     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_end                                                                                                               \t|  1.616e-06      \t|  2              \t|  3.232e-06      \t|  8.4532e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_epoch_end                                                                                                             \t|  1.605e-06      \t|  2              \t|  3.21e-06       \t|  8.3956e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_start             \t|  3.164e-06      \t|  1              \t|  3.164e-06      \t|  8.2753e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_start\t|  1.525e-06      \t|  2              \t|  3.05e-06       \t|  7.9772e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_start                                                                                                                 \t|  1.513e-06      \t|  2              \t|  3.026e-06      \t|  7.9144e-07     \t|\n",
      "|  [Callback]TQDMProgressBar.teardown                                                                                                                             \t|  2.974e-06      \t|  1              \t|  2.974e-06      \t|  7.7784e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_end                                                                                                                        \t|  2.97e-06       \t|  1              \t|  2.97e-06       \t|  7.7679e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_save_checkpoint                                                                                                                  \t|  2.789e-06      \t|  1              \t|  2.789e-06      \t|  7.2945e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_epoch_end                                                                                                                  \t|  2.354e-06      \t|  1              \t|  2.354e-06      \t|  6.1568e-07     \t|\n",
      "|  [Callback]ModelSummary.on_validation_epoch_start                                                                                                               \t|  1.118e-06      \t|  2              \t|  2.236e-06      \t|  5.8482e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_epoch_start                                                                                                           \t|  1.005e-06      \t|  2              \t|  2.01e-06       \t|  5.2571e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.setup                                                                                                                               \t|  1.944e-06      \t|  1              \t|  1.944e-06      \t|  5.0845e-07     \t|\n",
      "|  [Callback]ModelSummary.on_sanity_check_end                                                                                                                     \t|  1.814e-06      \t|  1              \t|  1.814e-06      \t|  4.7445e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_epoch_start                                                                                                                \t|  1.713e-06      \t|  1              \t|  1.713e-06      \t|  4.4803e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_fit_end                                                                                                                          \t|  1.686e-06      \t|  1              \t|  1.686e-06      \t|  4.4097e-07     \t|\n",
      "|  [Callback]ModelSummary.setup                                                                                                                                   \t|  1.669e-06      \t|  1              \t|  1.669e-06      \t|  4.3652e-07     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_start                                                                                                                  \t|  1.427e-06      \t|  1              \t|  1.427e-06      \t|  3.7323e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_start    \t|  1.281e-06      \t|  1              \t|  1.281e-06      \t|  3.3504e-07     \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.teardown                                                                                                            \t|  1.241e-06      \t|  1              \t|  1.241e-06      \t|  3.2458e-07     \t|\n",
      "|  [Callback]ModelSummary.teardown                                                                                                                                \t|  1.2e-06        \t|  1              \t|  1.2e-06        \t|  3.1386e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_fit_start                                                                                                                        \t|  1.191e-06      \t|  1              \t|  1.191e-06      \t|  3.115e-07      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_end      \t|  1.151e-06      \t|  1              \t|  1.151e-06      \t|  3.0104e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_end             \t|  1.15e-06       \t|  1              \t|  1.15e-06       \t|  3.0078e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.teardown                 \t|  1.118e-06      \t|  1              \t|  1.118e-06      \t|  2.9241e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_start     \t|  1.089e-06      \t|  1              \t|  1.089e-06      \t|  2.8482e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_end               \t|  1.089e-06      \t|  1              \t|  1.089e-06      \t|  2.8482e-07     \t|\n",
      "|  [Callback]ModelSummary.on_fit_end                                                                                                                              \t|  1.085e-06      \t|  1              \t|  1.085e-06      \t|  2.8378e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.teardown                                                                                                                            \t|  1.059e-06      \t|  1              \t|  1.059e-06      \t|  2.7698e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.configure_sharded_model                                                                                                             \t|  9.89e-07       \t|  1              \t|  9.89e-07       \t|  2.5867e-07     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_end                                                                                                                    \t|  9.73e-07       \t|  1              \t|  9.73e-07       \t|  2.5449e-07     \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1, profiler='simple')\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d47ac-b2b5-4a4a-9463-a78fe49f2f28",
   "metadata": {},
   "source": [
    "## Early stopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8584c6d-8367-4149-997d-622ae5c94d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "early_stop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    patience=3, \n",
    "    strict=False, \n",
    "    verbose=False, \n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa2a7e-badf-4eb2-9c80-a41830a2cac5",
   "metadata": {},
   "source": [
    "## Model checkpoint callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "671f4e6c-ee68-4586-b5aa-b1978593d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", \n",
    "    dirpath='models/', \n",
    "    filename='cat_vs_dogs-{epoch:02d}-{val_loss:.2f}', \n",
    "    save_top_k=3, #keep top 3 models\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4d3ec-f161-48e4-a3e6-bbc00e9b2cdb",
   "metadata": {},
   "source": [
    "## Custom callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cc552a5-72cd-4035-bc7d-68bfc4e9afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "class CustomPrintCallback(Callback):\n",
    "\n",
    "    def on_init_start(self, trainer):\n",
    "        print(f'on_init_start called, trainer: {trainer}')\n",
    "\n",
    "    def on_init_end(self, trainer):\n",
    "        print(f'on_init_end called, trainer: {trainer}')\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f'on_train_end called, trainer: {trainer}, module: {pl_module}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96108176-2a19-4254-8d1f-9a7cbcdfee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download and extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:858: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | conv_1 | Sequential | 448   \n",
      "1 | conv_2 | Sequential | 4.6 K \n",
      "2 | conv_3 | Sequential | 18.5 K\n",
      "3 | fc_1   | Sequential | 11.1 M\n",
      "4 | fc_2   | Sequential | 258   \n",
      "--------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.530    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created, split:\n",
      "training images: 19999\n",
      "validation images: 4999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9461f6ea4a014501b3c844ae0e4dcb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_train_end called, trainer: <lightning.pytorch.trainer.trainer.Trainer object at 0x7f97c66aefd0>, module: SimpleCNN(\n",
      "  (conv_1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_1): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=43264, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (fc_2): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1, callbacks=[early_stop_cb, checkpoint_cb, CustomPrintCallback()])\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bd54f51-263b-4dd5-8d5b-6f3a4f236153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score: 0.8199627995491028, path:/app/notebooks/models/cat_vs_dogs-epoch=00-val_loss=0.82.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(f'Best model score: {checkpoint_cb.best_model_score}, path:{checkpoint_cb.best_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182a1cd-0fec-4dea-a8b6-e218b4c830f9",
   "metadata": {},
   "source": [
    "## Restore best saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462aed69-d532-431c-b59c-c94fb5239dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv_1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_1): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=43264, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (fc_2): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = SimpleCNN.load_from_checkpoint(batch_size=128, learning_rate=0.001, checkpoint_path = checkpoint_cb.best_model_path)\n",
    "trained_model = trained_model.to(device)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ad556-a1fe-4219-a61e-df6b7a755f50",
   "metadata": {},
   "source": [
    "## Export model in different formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b83ef-8e90-4164-87df-ce5162e84348",
   "metadata": {},
   "source": [
    "### torchscript\n",
    "https://pytorch.org/docs/stable/jit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03bf5ff6-bfff-41c1-a313-602ff10163dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_model = trained_model.to_torchscript()\n",
    "torch.jit.save(script_model, \"models/simple_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9ab5b-8e12-42d1-989e-6e19f29eff89",
   "metadata": {},
   "source": [
    "### ONNX\n",
    "ONNX aims to be a framework-agnostic standardformat for deep-learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52de6084-838c-4919-9540-cd3c9de0a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_model = SimpleCNN.load_from_checkpoint(batch_size=128, learning_rate=0.001, checkpoint_path = 'models/cat_vs_dogs-epoch=00-val_loss=0.82.ckpt')\n",
    "trained_model = trained_model.to(device)\n",
    "trained_model.eval()\n",
    "\n",
    "input_sample = torch.randn(1, 3, 224, 224)\n",
    "trained_model.to_onnx(\"models/simple_cnn.onnx\", input_sample, export_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a178cbe-a8b3-4a63-8461-2593ef95dbc8",
   "metadata": {},
   "source": [
    "We can then use https://netron.app/ to visualize the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b25863-4ff0-4119-840f-20d8f7fe64f7",
   "metadata": {},
   "source": [
    "## Validate servability of model\n",
    "With lightning we can validate our model is production-ready even before training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d83b5fcf-f2bc-47fa-a6dc-aa4bcd4c73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.serve import ServableModule, ServableModuleValidator\n",
    "from typing import Dict, Optional\n",
    "\n",
    "class ProductionReadyModel(SimpleCNN, ServableModule):\n",
    "    def configure_payload(self):\n",
    "        # 1: Access the train dataloader and load a single sample.\n",
    "        image, _ = self.trainer.train_dataloader.dataset[0]\n",
    "\n",
    "        # 2: Convert the image into a PIL Image to bytes and encode it with base64\n",
    "        pil_image = T.ToPILImage()(image)\n",
    "        buffered = BytesIO()\n",
    "        pil_image.save(buffered, format=\"JPEG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode(\"UTF-8\")\n",
    "\n",
    "        return {\"body\": {\"x\": img_str}}\n",
    "\n",
    "    def configure_serialization(self):\n",
    "        return {\"x\": Image(224, 224).deserialize}, {\"output\": Top1().serialize}\n",
    "\n",
    "    def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        return {\"output\": self.model(x)}\n",
    "\n",
    "    def configure_response(self):\n",
    "        return {\"output\": 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed151f4-28c0-4028-bf75-7822fb4b50c3",
   "metadata": {},
   "source": [
    "Note: The following code will only work in a script called from a CLI as the ServableModuleValidator is designed to work from the CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f74fe836-3508-4f2c-8db6-40e576cea999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.cli import LightningCLI\n",
    "#cli = LightningCLI(\n",
    "#    ProductionReadyModel,\n",
    "#    CatDogImageDataModule,\n",
    "#    seed_everything_default=42,\n",
    "#    save_config_kwargs={\"overwrite\": True},\n",
    "#    run=False,\n",
    "#    trainer_defaults={\n",
    "#        \"accelerator\": \"cpu\",\n",
    "#        \"callbacks\": [ServableModuleValidator()],\n",
    "#        \"max_epochs\": 1,\n",
    "#        \"limit_train_batches\": 5,\n",
    "#        \"limit_val_batches\": 5,\n",
    "#    },\n",
    "#)\n",
    "#cli.trainer.fit(cli.model, cli.datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
