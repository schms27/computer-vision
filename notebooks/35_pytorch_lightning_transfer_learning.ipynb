{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562b56de-d1f0-46cd-a5d6-9decf518eb51",
   "metadata": {},
   "source": [
    "# PyTorch Transfer learning\n",
    "\n",
    "We use PyTorch Lightning to demonstrate transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1edf75-99e3-40ed-984b-2d182849d6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "\n",
    "# check if cuda is usable\n",
    "if torch.cuda.is_available():\n",
    "  device = 'cuda' \n",
    "else:\n",
    "  device = 'cpu' \n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c597b25-679c-44cc-8cc6-6de99ecfb0b9",
   "metadata": {},
   "source": [
    "## Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c05b983-5595-41d8-9562-e206bbea28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDogImageDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dl_path='./tmp', batch_size = 32, num_workers=0, cache_dataset=True):\n",
    "        super().__init__()\n",
    "        self._dl_path = dl_path\n",
    "        self.batch_size = batch_size\n",
    "        self._num_workers = num_workers\n",
    "        self._cache_dataset = cache_dataset\n",
    "\n",
    "    @property\n",
    "    def data_path(self):\n",
    "        return Path(self._dl_path).joinpath(\"PetImages\")\n",
    "\n",
    "    @property\n",
    "    def normalize_transform(self):\n",
    "        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    @property\n",
    "    def train_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            self.normalize_transform,\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def val_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)), \n",
    "            transforms.ToTensor(), \n",
    "            self.normalize_transform\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Download images and prepare images datasets.\"\"\"\n",
    "        url = 'https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip'  \n",
    "        if not os.path.exists(self.data_path):\n",
    "            os.makedirs(self.data_path)\n",
    "        if len(os.listdir(self.data_path)) == 0:\n",
    "            download_and_extract_archive(url=url, download_root=self._dl_path, remove_finished=not self._cache_dataset)\n",
    "        else:\n",
    "            print(\"Dataset already exists, skipping download and extraction...\")\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        # make assignments here (val/train/test split)\n",
    "        dataset = self.create_dataset(self.data_path, self.train_transform)\n",
    "        self.train_data, self.val_data = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "        print(\"Dataset created, split:\")\n",
    "        print(f'training images: {len(self.train_data)}')\n",
    "        print(f'validation images: {len(self.val_data)}')\n",
    "\n",
    "    def create_dataset(self, root, transform):\n",
    "        return ImageFolder(root=root, transform=transform, is_valid_file=self._is_image_valid)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset=self.train_data, batch_size=self.batch_size, num_workers=self._num_workers, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset=self.val_data, batch_size=self.batch_size, num_workers=self._num_workers, shuffle=False)\n",
    "\n",
    "    def _is_image_valid(self, image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caa1a91-9047-468f-8a36-4a5fc242290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = CatDogImageDataModule(num_workers=16, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e61e4a8-b2b3-48c6-96bb-d9176d6b90f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download and extraction...\n"
     ]
    }
   ],
   "source": [
    "# The following methods will be called by the trainer automatically before training:\n",
    "dm.prepare_data()\n",
    "#dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8e92f-c7cb-4821-bd2d-6da2b2e23da8",
   "metadata": {},
   "source": [
    "## Build Lightning transfer learning model\n",
    "We use resnet as a backbone here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8eda0e1-a4fc-4b4c-ba8e-14b735f6bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "\n",
    "class ImagenetTransferLearnModule(pl.LightningModule):\n",
    "    def __init__(self, lr=0.02, num_of_target_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes = num_of_target_classes)\n",
    "\n",
    "        backbone = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "        num_of_filters = backbone.fc.in_features\n",
    "        layers = list(backbone.children())[:-1]\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "\n",
    "        self.head = nn.Linear(num_of_filters, num_of_target_classes)\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "      return F.nll_loss(logits, labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, label = batch\n",
    "        output = self.forward(data)\n",
    "        loss = nn.CrossEntropyLoss()(output,label)\n",
    "        self.log('train_loss', loss)\n",
    "        return {'loss': loss, 'log': self.log}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_data, val_label = batch\n",
    "        val_output = self.forward(val_data)\n",
    "        val_loss = nn.CrossEntropyLoss()(val_output, val_label)\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.feature_extractor.eval()\n",
    "        with torch.no_grad():\n",
    "            representations = self.feature_extractor(x).flatten(1)\n",
    "        x = self.head(representations)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0410169-183d-4e71-b23f-18c067154005",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d22a770-d8b7-46de-93fa-38a8c3926a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /home/user/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n",
      "100%|██████████| 230M/230M [00:27<00:00, 8.73MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = ImagenetTransferLearnModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69337d-dadb-4bd0-8528-da158ede7129",
   "metadata": {},
   "source": [
    "## Setup trainer and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "103dde40-910a-4669-95a9-95a09c58b0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download and extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:858: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created, split:\n",
      "training images: 19999\n",
      "validation images: 4999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | accuracy          | MulticlassAccuracy | 0     \n",
      "1 | feature_extractor | Sequential         | 58.1 M\n",
      "2 | head              | Linear             | 4.1 K \n",
      "---------------------------------------------------------\n",
      "58.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "58.1 M    Total params\n",
      "232.592   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8032d1d9fe364a56a959e7f395fff2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=20)\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7cb24-b98e-4d99-8a00-e4535afb447d",
   "metadata": {},
   "source": [
    "## Tensorboard logs\n",
    "We can use tensorboard to visualize our training metrics.\n",
    "\n",
    "We can either manually start the tensorboard server and go to [localhost:6006](http://localhost:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea0e871b-f83e-4629-b851-22329ed6d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir='./lightning_logs' --bind_all & true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1203a-861a-4dbc-b222-8df1de30ec5a",
   "metadata": {},
   "source": [
    "The other possibility is to use the tensorboard extension directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5f8c0d-f55e-4fd4-b05e-fb4358bbeacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bacf954802c3e7c9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bacf954802c3e7c9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53ff99-5456-4097-af55-a1f39344295d",
   "metadata": {},
   "source": [
    "## Use profiler to find bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "858ac0f4-9352-4862-94d0-b66bdab29a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download and extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | conv_1 | Sequential | 448   \n",
      "1 | conv_2 | Sequential | 4.6 K \n",
      "2 | conv_3 | Sequential | 18.5 K\n",
      "3 | fc_1   | Sequential | 11.1 M\n",
      "4 | fc_2   | Sequential | 258   \n",
      "--------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.530    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created, split:\n",
      "training images: 19999\n",
      "validation images: 4999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bc0a3ba9154a289937b3d1191526a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "FIT Profiler Report\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Action                                                                                                                                                         \t|  Mean duration (s)\t|  Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Total                                                                                                                                                          \t|  -              \t|  6336           \t|  382.34         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  run_training_epoch                                                                                                                                             \t|  199.86         \t|  1              \t|  199.86         \t|  52.273         \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.setup                                                                                                               \t|  165.54         \t|  1              \t|  165.54         \t|  43.296         \t|\n",
      "|  run_training_batch                                                                                                                                             \t|  1.0434         \t|  157            \t|  163.81         \t|  42.844         \t|\n",
      "|  [LightningModule]SimpleCNN.optimizer_step                                                                                                                      \t|  1.0432         \t|  157            \t|  163.78         \t|  42.837         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.backward                                                                                                                        \t|  0.56283        \t|  157            \t|  88.365         \t|  23.112         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.training_step                                                                                                                   \t|  0.45052        \t|  157            \t|  70.731         \t|  18.499         \t|\n",
      "|  [_EvaluationLoop].val_next                                                                                                                                     \t|  0.44695        \t|  43             \t|  19.219         \t|  5.0266         \t|\n",
      "|  [Strategy]SingleDeviceStrategy.validation_step                                                                                                                 \t|  0.4565         \t|  42             \t|  19.173         \t|  5.0146         \t|\n",
      "|  [_TrainingEpochLoop].train_dataloader_next                                                                                                                     \t|  0.045153       \t|  157            \t|  7.089          \t|  1.8541         \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_end       \t|  0.29569        \t|  1              \t|  0.29569        \t|  0.077335       \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_end                                                                                                                   \t|  0.0015746      \t|  157            \t|  0.24722        \t|  0.064659       \t|\n",
      "|  [LightningModule]SimpleCNN.optimizer_zero_grad                                                                                                                 \t|  0.00079984     \t|  157            \t|  0.12557        \t|  0.032843       \t|\n",
      "|  [Strategy]SingleDeviceStrategy.batch_to_device                                                                                                                 \t|  0.00023259     \t|  199            \t|  0.046285       \t|  0.012106       \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_batch_end                                                                                                              \t|  0.0010914      \t|  42             \t|  0.04584        \t|  0.011989       \t|\n",
      "|  [LightningModule]SimpleCNN.transfer_batch_to_device                                                                                                            \t|  0.00012917     \t|  199            \t|  0.025704       \t|  0.0067228      \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_start                                                                                                                  \t|  0.0066992      \t|  2              \t|  0.013398       \t|  0.0035043      \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_batch_start                                                                                                            \t|  0.00026789     \t|  42             \t|  0.011251       \t|  0.0029428      \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.prepare_data                                                                                                        \t|  0.010801       \t|  1              \t|  0.010801       \t|  0.0028249      \t|\n",
      "|  [Callback]TQDMProgressBar.on_sanity_check_start                                                                                                                \t|  0.0090482      \t|  1              \t|  0.0090482      \t|  0.0023665      \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_start                                                                                                                       \t|  0.0077118      \t|  1              \t|  0.0077118      \t|  0.002017       \t|\n",
      "|  [LightningModule]SimpleCNN.configure_gradient_clipping                                                                                                         \t|  4.4046e-05     \t|  157            \t|  0.0069153      \t|  0.0018087      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_end       \t|  3.7259e-05     \t|  157            \t|  0.0058497      \t|  0.00153        \t|\n",
      "|  [Callback]ModelSummary.on_fit_start                                                                                                                            \t|  0.00262        \t|  1              \t|  0.00262        \t|  0.00068524     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_end                                                                                                                    \t|  0.0011806      \t|  2              \t|  0.0023612      \t|  0.00061757     \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_end                                                                                                                      \t|  1.0223e-05     \t|  157            \t|  0.001605       \t|  0.00041979     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.setup                    \t|  0.0015413      \t|  1              \t|  0.0015413      \t|  0.00040313     \t|\n",
      "|  [Callback]TQDMProgressBar.on_after_backward                                                                                                                    \t|  9.0288e-06     \t|  157            \t|  0.0014175      \t|  0.00037075     \t|\n",
      "|  [LightningModule]SimpleCNN.on_before_batch_transfer                                                                                                            \t|  4.9639e-06     \t|  199            \t|  0.00098782     \t|  0.00025836     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_model_eval                                                                                                            \t|  0.0004933      \t|  2              \t|  0.0009866      \t|  0.00025804     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_start                                                                                                                 \t|  5.8494e-06     \t|  157            \t|  0.00091836     \t|  0.00024019     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_zero_grad                                                                                                                  \t|  4.8144e-06     \t|  157            \t|  0.00075585     \t|  0.00019769     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_backward                                                                                                                   \t|  4.3077e-06     \t|  157            \t|  0.0006763      \t|  0.00017688     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_optimizer_step                                                                                                             \t|  3.9552e-06     \t|  157            \t|  0.00062096     \t|  0.00016241     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_backward       \t|  3.9271e-06     \t|  157            \t|  0.00061655     \t|  0.00016126     \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.val_dataloader                                                                                                      \t|  0.00060321     \t|  1              \t|  0.00060321     \t|  0.00015777     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_start                                                                                                                 \t|  0.00059927     \t|  1              \t|  0.00059927     \t|  0.00015674     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_start     \t|  3.2965e-06     \t|  157            \t|  0.00051755     \t|  0.00013536     \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_start                                                                                                                    \t|  3.2854e-06     \t|  157            \t|  0.00051581     \t|  0.00013491     \t|\n",
      "|  [LightningModule]SimpleCNN.on_after_batch_transfer                                                                                                             \t|  2.5381e-06     \t|  199            \t|  0.00050509     \t|  0.0001321      \t|\n",
      "|  [Callback]ModelSummary.on_before_zero_grad                                                                                                                     \t|  3.0525e-06     \t|  157            \t|  0.00047925     \t|  0.00012535     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_end                                                                                                                         \t|  0.00046157     \t|  1              \t|  0.00046157     \t|  0.00012072     \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.train_dataloader                                                                                                    \t|  0.00040584     \t|  1              \t|  0.00040584     \t|  0.00010614     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_end                                                                                                                   \t|  0.00039762     \t|  1              \t|  0.00039762     \t|  0.000104       \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_after_backward        \t|  2.4104e-06     \t|  157            \t|  0.00037843     \t|  9.8978e-05     \t|\n",
      "|  [Callback]ModelSummary.on_after_backward                                                                                                                       \t|  2.3845e-06     \t|  157            \t|  0.00037436     \t|  9.7914e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_batch_start                                                                                                                \t|  2.3571e-06     \t|  157            \t|  0.00037006     \t|  9.6789e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_batch_end                                                                                                                  \t|  2.3416e-06     \t|  157            \t|  0.00036764     \t|  9.6154e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_before_zero_grad                                                                                                                 \t|  2.3214e-06     \t|  157            \t|  0.00036446     \t|  9.5322e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_zero_grad      \t|  2.2854e-06     \t|  157            \t|  0.00035881     \t|  9.3845e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_after_backward                                                                                                                   \t|  2.2716e-06     \t|  157            \t|  0.00035665     \t|  9.328e-05      \t|\n",
      "|  [Callback]ModelSummary.on_before_backward                                                                                                                      \t|  2.1938e-06     \t|  157            \t|  0.00034442     \t|  9.0082e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_model_train                                                                                                           \t|  0.00016758     \t|  2              \t|  0.00033517     \t|  8.7662e-05     \t|\n",
      "|  [Callback]ModelSummary.on_validation_batch_end                                                                                                                 \t|  7.7705e-06     \t|  42             \t|  0.00032636     \t|  8.5359e-05     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_batch_start                                                                                                            \t|  2.0699e-06     \t|  157            \t|  0.00032498     \t|  8.4996e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_optimizer_step \t|  2.0514e-06     \t|  157            \t|  0.00032206     \t|  8.4234e-05     \t|\n",
      "|  [Callback]ModelSummary.on_before_optimizer_step                                                                                                                \t|  2.0042e-06     \t|  157            \t|  0.00031466     \t|  8.2298e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_before_backward                                                                                                                  \t|  1.7974e-06     \t|  157            \t|  0.00028219     \t|  7.3807e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.configure_optimizers                                                                                                                \t|  0.00022603     \t|  1              \t|  0.00022603     \t|  5.9118e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_before_optimizer_step                                                                                                            \t|  1.2659e-06     \t|  157            \t|  0.00019875     \t|  5.1982e-05     \t|\n",
      "|  [Callback]ModelSummary.on_validation_batch_start                                                                                                               \t|  4.1054e-06     \t|  42             \t|  0.00017243     \t|  4.5098e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_end        \t|  8.4052e-05     \t|  2              \t|  0.0001681      \t|  4.3967e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_end  \t|  2.4945e-06     \t|  42             \t|  0.00010477     \t|  2.7402e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_start\t|  2.4063e-06     \t|  42             \t|  0.00010107     \t|  2.6433e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_batch_end                                                                                                             \t|  2.3547e-06     \t|  42             \t|  9.8897e-05     \t|  2.5866e-05     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_batch_start                                                                                                           \t|  2.0254e-06     \t|  42             \t|  8.5067e-05     \t|  2.2249e-05     \t|\n",
      "|  [Callback]ModelSummary.on_save_checkpoint                                                                                                                      \t|  3.6856e-05     \t|  1              \t|  3.6856e-05     \t|  9.6396e-06     \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_end                                                                                                                      \t|  2.6706e-05     \t|  1              \t|  2.6706e-05     \t|  6.9849e-06     \t|\n",
      "|  [Callback]ModelSummary.on_validation_start                                                                                                                     \t|  8.923e-06      \t|  2              \t|  1.7846e-05     \t|  4.6676e-06     \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.state_dict                                                                                                          \t|  1.6474e-05     \t|  1              \t|  1.6474e-05     \t|  4.3087e-06     \t|\n",
      "|  [Callback]ModelSummary.on_validation_end                                                                                                                       \t|  5.533e-06      \t|  2              \t|  1.1066e-05     \t|  2.8943e-06     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_start                                                                                                             \t|  5.406e-06      \t|  2              \t|  1.0812e-05     \t|  2.8278e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_sanity_check_end                                                                                                                  \t|  9.564e-06      \t|  1              \t|  9.564e-06      \t|  2.5014e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_epoch_end                                                                                                              \t|  4.285e-06      \t|  2              \t|  8.57e-06       \t|  2.2415e-06     \t|\n",
      "|  [Callback]ModelSummary.on_sanity_check_start                                                                                                                   \t|  7.48e-06       \t|  1              \t|  7.48e-06       \t|  1.9564e-06     \t|\n",
      "|  [Callback]ModelSummary.on_train_start                                                                                                                          \t|  7.47e-06       \t|  1              \t|  7.47e-06       \t|  1.9538e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_save_checkpoint                                                                                                                   \t|  7.412e-06      \t|  1              \t|  7.412e-06      \t|  1.9386e-06     \t|\n",
      "|  [LightningModule]SimpleCNN.configure_callbacks                                                                                                                 \t|  6.752e-06      \t|  1              \t|  6.752e-06      \t|  1.766e-06      \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_start                                                                                                                      \t|  6.024e-06      \t|  1              \t|  6.024e-06      \t|  1.5756e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.setup                                                                                                                                \t|  5.458e-06      \t|  1              \t|  5.458e-06      \t|  1.4275e-06     \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_start                                                                                                                    \t|  4.828e-06      \t|  1              \t|  4.828e-06      \t|  1.2627e-06     \t|\n",
      "|  [LightningModule]SimpleCNN.prepare_data                                                                                                                        \t|  4.716e-06      \t|  1              \t|  4.716e-06      \t|  1.2335e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_start      \t|  2.2855e-06     \t|  2              \t|  4.571e-06      \t|  1.1955e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_end  \t|  2.2675e-06     \t|  2              \t|  4.535e-06      \t|  1.1861e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_start           \t|  4.469e-06      \t|  1              \t|  4.469e-06      \t|  1.1689e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_end                                                                                                                           \t|  4.356e-06      \t|  1              \t|  4.356e-06      \t|  1.1393e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_epoch_start                                                                                                            \t|  1.989e-06      \t|  2              \t|  3.978e-06      \t|  1.0404e-06     \t|\n",
      "|  [Callback]ModelSummary.on_train_end                                                                                                                            \t|  3.877e-06      \t|  1              \t|  3.877e-06      \t|  1.014e-06      \t|\n",
      "|  [Callback]ModelSummary.on_validation_epoch_end                                                                                                                 \t|  1.93e-06       \t|  2              \t|  3.86e-06       \t|  1.0096e-06     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_end                                                                                                                   \t|  1.878e-06      \t|  2              \t|  3.756e-06      \t|  9.8237e-07     \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_start                                                                                                                         \t|  3.336e-06      \t|  1              \t|  3.336e-06      \t|  8.7252e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_save_checkpoint       \t|  3.26e-06       \t|  1              \t|  3.26e-06       \t|  8.5264e-07     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_end                                                                                                               \t|  1.616e-06      \t|  2              \t|  3.232e-06      \t|  8.4532e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_epoch_end                                                                                                             \t|  1.605e-06      \t|  2              \t|  3.21e-06       \t|  8.3956e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_start             \t|  3.164e-06      \t|  1              \t|  3.164e-06      \t|  8.2753e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_start\t|  1.525e-06      \t|  2              \t|  3.05e-06       \t|  7.9772e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_start                                                                                                                 \t|  1.513e-06      \t|  2              \t|  3.026e-06      \t|  7.9144e-07     \t|\n",
      "|  [Callback]TQDMProgressBar.teardown                                                                                                                             \t|  2.974e-06      \t|  1              \t|  2.974e-06      \t|  7.7784e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_end                                                                                                                        \t|  2.97e-06       \t|  1              \t|  2.97e-06       \t|  7.7679e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_save_checkpoint                                                                                                                  \t|  2.789e-06      \t|  1              \t|  2.789e-06      \t|  7.2945e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_epoch_end                                                                                                                  \t|  2.354e-06      \t|  1              \t|  2.354e-06      \t|  6.1568e-07     \t|\n",
      "|  [Callback]ModelSummary.on_validation_epoch_start                                                                                                               \t|  1.118e-06      \t|  2              \t|  2.236e-06      \t|  5.8482e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_validation_epoch_start                                                                                                           \t|  1.005e-06      \t|  2              \t|  2.01e-06       \t|  5.2571e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.setup                                                                                                                               \t|  1.944e-06      \t|  1              \t|  1.944e-06      \t|  5.0845e-07     \t|\n",
      "|  [Callback]ModelSummary.on_sanity_check_end                                                                                                                     \t|  1.814e-06      \t|  1              \t|  1.814e-06      \t|  4.7445e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_train_epoch_start                                                                                                                \t|  1.713e-06      \t|  1              \t|  1.713e-06      \t|  4.4803e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_fit_end                                                                                                                          \t|  1.686e-06      \t|  1              \t|  1.686e-06      \t|  4.4097e-07     \t|\n",
      "|  [Callback]ModelSummary.setup                                                                                                                                   \t|  1.669e-06      \t|  1              \t|  1.669e-06      \t|  4.3652e-07     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_start                                                                                                                  \t|  1.427e-06      \t|  1              \t|  1.427e-06      \t|  3.7323e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_start    \t|  1.281e-06      \t|  1              \t|  1.281e-06      \t|  3.3504e-07     \t|\n",
      "|  [LightningDataModule]CatDogImageDataModule.teardown                                                                                                            \t|  1.241e-06      \t|  1              \t|  1.241e-06      \t|  3.2458e-07     \t|\n",
      "|  [Callback]ModelSummary.teardown                                                                                                                                \t|  1.2e-06        \t|  1              \t|  1.2e-06        \t|  3.1386e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.on_fit_start                                                                                                                        \t|  1.191e-06      \t|  1              \t|  1.191e-06      \t|  3.115e-07      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_end      \t|  1.151e-06      \t|  1              \t|  1.151e-06      \t|  3.0104e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_end             \t|  1.15e-06       \t|  1              \t|  1.15e-06       \t|  3.0078e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.teardown                 \t|  1.118e-06      \t|  1              \t|  1.118e-06      \t|  2.9241e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_start     \t|  1.089e-06      \t|  1              \t|  1.089e-06      \t|  2.8482e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_end               \t|  1.089e-06      \t|  1              \t|  1.089e-06      \t|  2.8482e-07     \t|\n",
      "|  [Callback]ModelSummary.on_fit_end                                                                                                                              \t|  1.085e-06      \t|  1              \t|  1.085e-06      \t|  2.8378e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.teardown                                                                                                                            \t|  1.059e-06      \t|  1              \t|  1.059e-06      \t|  2.7698e-07     \t|\n",
      "|  [LightningModule]SimpleCNN.configure_sharded_model                                                                                                             \t|  9.89e-07       \t|  1              \t|  9.89e-07       \t|  2.5867e-07     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_end                                                                                                                    \t|  9.73e-07       \t|  1              \t|  9.73e-07       \t|  2.5449e-07     \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1, profiler='simple')\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d47ac-b2b5-4a4a-9463-a78fe49f2f28",
   "metadata": {},
   "source": [
    "## Early stopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8584c6d-8367-4149-997d-622ae5c94d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "early_stop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    patience=3, \n",
    "    strict=False, \n",
    "    verbose=False, \n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa2a7e-badf-4eb2-9c80-a41830a2cac5",
   "metadata": {},
   "source": [
    "## Model checkpoint callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "671f4e6c-ee68-4586-b5aa-b1978593d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", \n",
    "    dirpath='models/', \n",
    "    filename='cat_vs_dogs-{epoch:02d}-{val_loss:.2f}', \n",
    "    save_top_k=3, #keep top 3 models\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4d3ec-f161-48e4-a3e6-bbc00e9b2cdb",
   "metadata": {},
   "source": [
    "## Custom callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cc552a5-72cd-4035-bc7d-68bfc4e9afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "class CustomPrintCallback(Callback):\n",
    "\n",
    "    def on_init_start(self, trainer):\n",
    "        print(f'on_init_start called, trainer: {trainer}')\n",
    "\n",
    "    def on_init_end(self, trainer):\n",
    "        print(f'on_init_end called, trainer: {trainer}')\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f'on_train_end called, trainer: {trainer}, module: {pl_module}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96108176-2a19-4254-8d1f-9a7cbcdfee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download and extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:858: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | conv_1 | Sequential | 448   \n",
      "1 | conv_2 | Sequential | 4.6 K \n",
      "2 | conv_3 | Sequential | 18.5 K\n",
      "3 | fc_1   | Sequential | 11.1 M\n",
      "4 | fc_2   | Sequential | 258   \n",
      "--------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.530    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created, split:\n",
      "training images: 19999\n",
      "validation images: 4999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9461f6ea4a014501b3c844ae0e4dcb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_train_end called, trainer: <lightning.pytorch.trainer.trainer.Trainer object at 0x7f97c66aefd0>, module: SimpleCNN(\n",
      "  (conv_1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_1): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=43264, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (fc_2): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1, callbacks=[early_stop_cb, checkpoint_cb, CustomPrintCallback()])\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bd54f51-263b-4dd5-8d5b-6f3a4f236153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score: 0.8199627995491028, path:/app/notebooks/models/cat_vs_dogs-epoch=00-val_loss=0.82.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(f'Best model score: {checkpoint_cb.best_model_score}, path:{checkpoint_cb.best_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182a1cd-0fec-4dea-a8b6-e218b4c830f9",
   "metadata": {},
   "source": [
    "## Restore best saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462aed69-d532-431c-b59c-c94fb5239dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv_1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_1): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=43264, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (fc_2): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = SimpleCNN.load_from_checkpoint(batch_size=128, learning_rate=0.001, checkpoint_path = checkpoint_cb.best_model_path)\n",
    "trained_model = trained_model.to(device)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ad556-a1fe-4219-a61e-df6b7a755f50",
   "metadata": {},
   "source": [
    "## Export model in different formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b83ef-8e90-4164-87df-ce5162e84348",
   "metadata": {},
   "source": [
    "### torchscript\n",
    "https://pytorch.org/docs/stable/jit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03bf5ff6-bfff-41c1-a313-602ff10163dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_model = trained_model.to_torchscript()\n",
    "torch.jit.save(script_model, \"models/simple_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9ab5b-8e12-42d1-989e-6e19f29eff89",
   "metadata": {},
   "source": [
    "### ONNX\n",
    "ONNX aims to be a framework-agnostic standardformat for deep-learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52de6084-838c-4919-9540-cd3c9de0a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_model = SimpleCNN.load_from_checkpoint(batch_size=128, learning_rate=0.001, checkpoint_path = 'models/cat_vs_dogs-epoch=00-val_loss=0.82.ckpt')\n",
    "trained_model = trained_model.to(device)\n",
    "trained_model.eval()\n",
    "\n",
    "input_sample = torch.randn(1, 3, 224, 224)\n",
    "trained_model.to_onnx(\"models/simple_cnn.onnx\", input_sample, export_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a178cbe-a8b3-4a63-8461-2593ef95dbc8",
   "metadata": {},
   "source": [
    "We can then use https://netron.app/ to visualize the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b25863-4ff0-4119-840f-20d8f7fe64f7",
   "metadata": {},
   "source": [
    "## Validate servability of model\n",
    "With lightning we can validate our model is production-ready even before training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d83b5fcf-f2bc-47fa-a6dc-aa4bcd4c73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.serve import ServableModule, ServableModuleValidator\n",
    "from typing import Dict, Optional\n",
    "\n",
    "class ProductionReadyModel(SimpleCNN, ServableModule):\n",
    "    def configure_payload(self):\n",
    "        # 1: Access the train dataloader and load a single sample.\n",
    "        image, _ = self.trainer.train_dataloader.dataset[0]\n",
    "\n",
    "        # 2: Convert the image into a PIL Image to bytes and encode it with base64\n",
    "        pil_image = T.ToPILImage()(image)\n",
    "        buffered = BytesIO()\n",
    "        pil_image.save(buffered, format=\"JPEG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode(\"UTF-8\")\n",
    "\n",
    "        return {\"body\": {\"x\": img_str}}\n",
    "\n",
    "    def configure_serialization(self):\n",
    "        return {\"x\": Image(224, 224).deserialize}, {\"output\": Top1().serialize}\n",
    "\n",
    "    def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        return {\"output\": self.model(x)}\n",
    "\n",
    "    def configure_response(self):\n",
    "        return {\"output\": 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed151f4-28c0-4028-bf75-7822fb4b50c3",
   "metadata": {},
   "source": [
    "Note: The following code will only work in a script called from a CLI as the ServableModuleValidator is designed to work from the CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f74fe836-3508-4f2c-8db6-40e576cea999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.cli import LightningCLI\n",
    "#cli = LightningCLI(\n",
    "#    ProductionReadyModel,\n",
    "#    CatDogImageDataModule,\n",
    "#    seed_everything_default=42,\n",
    "#    save_config_kwargs={\"overwrite\": True},\n",
    "#    run=False,\n",
    "#    trainer_defaults={\n",
    "#        \"accelerator\": \"cpu\",\n",
    "#        \"callbacks\": [ServableModuleValidator()],\n",
    "#        \"max_epochs\": 1,\n",
    "#        \"limit_train_batches\": 5,\n",
    "#        \"limit_val_batches\": 5,\n",
    "#    },\n",
    "#)\n",
    "#cli.trainer.fit(cli.model, cli.datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
